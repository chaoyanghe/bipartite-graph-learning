Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:14] INFO BipartiteGraphDataLoader __init__().
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:21] INFO group_u_list_file_path = ./../data/Tencent-QQ/node_list
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:22] INFO group_u_attr_file_path = ./../data/Tencent-QQ/node_attr
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:23] INFO group_u_label_file_path = ./../data/Tencent-QQ/node_true
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:25] INFO edge_list_file_path = ./../data/Tencent-QQ/edgelist
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:27] INFO group_v_list_file_path = ./../data/Tencent-QQ/group_list
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:28] INFO group_v_attr_file_path = ./../data/Tencent-QQ/group_attr
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:29] INFO group_v_label_file_path = None
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:55] INFO BipartiteGraphDataLoader __init__(). END
Fri, 17 May 2019 15:38:16 bipartite_graph_data_loader.py[line:73] INFO ##### generate_adjacent_matrix_feature_and_labels. START
Fri, 17 May 2019 15:38:33 bipartite_graph_data_loader.py[line:170] INFO before merging with u_list, the len is = 1089436
Fri, 17 May 2019 15:38:34 bipartite_graph_data_loader.py[line:178] INFO after merging with u_list, the len is = 1089436
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:248] INFO count_no_attribute = 89682
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:249] INFO count_10 = 124
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:250] INFO count_14 = 134
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:251] INFO count_15 = 18997
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:252] INFO count_16 = 30893
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:253] INFO count_17 = 40023
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:254] INFO count_more_than_17 = 0
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:255] INFO count_all = 179853
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:273] INFO before merging with v_list, the len is = 90047
Fri, 17 May 2019 15:38:35 bipartite_graph_data_loader.py[line:281] INFO after merging with v_list, the len is = 90047
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:93] INFO raw edge_list len = 1979756
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:94] INFO edge_list len = 991734
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:295] INFO group U length = 619030
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:296] INFO group V length = 90044
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:100] INFO u_list len = 619030. [1, 139467, 253828, 358861, 461403, 338688, 262114, 758671, 855102, 951204, 1045871, 1140045, 1062918]
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:101] INFO v_list len = 90044. [0, 48809, 115451, 201816, 309872, 441355, 600161, 795107, 1017863, 1268422]
Fri, 17 May 2019 15:38:38 bipartite_graph_data_loader.py[line:302] INFO before filter, the len is = 1089436
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:306] INFO after filter, the len is = 619030
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:302] INFO before filter, the len is = 90047
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:306] INFO after filter, the len is = 90044
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:310] INFO __generate_adjacent_matrix START
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:312] INFO u_node_list = 619030
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:313] INFO v_node_list = 90044
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:314] INFO edge_list = 991734
Fri, 17 May 2019 15:38:39 bipartite_graph_data_loader.py[line:316] INFO start to load bipartite for u
Fri, 17 May 2019 15:38:52 bipartite_graph_data_loader.py[line:326] INFO (619030, 90044)
Fri, 17 May 2019 15:38:53 bipartite_graph_data_loader.py[line:329] INFO end to load bipartite for u
Fri, 17 May 2019 15:38:53 bipartite_graph_data_loader.py[line:331] INFO start to load bipartite for u
Fri, 17 May 2019 15:39:03 bipartite_graph_data_loader.py[line:341] INFO (90044, 619030)
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:344] INFO end to load bipartite for u
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:386] INFO u number: 619030
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:387] INFO u_adjacent_matrix: (619030, 90044)
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:390] INFO v number: 90044
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:391] INFO v_adjacent_matrix: (90044, 619030)
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:394] INFO batch_num_u = 619031
Fri, 17 May 2019 15:39:04 bipartite_graph_data_loader.py[line:397] INFO batch_num_v = 90045
Fri, 17 May 2019 15:39:50 bipartite_graph_data_loader.py[line:120] INFO #### generate_adjacent_matrix_feature_and_labels. END
Fri, 17 May 2019 15:39:50 input_data.py[line:74] INFO ####### Done bipartite graph loader #########
Fri, 17 May 2019 15:39:50 input_data.py[line:79] INFO u_attr len = 619030
Fri, 17 May 2019 15:39:50 input_data.py[line:80] INFO u_adj shape = (619030, 90044)
Fri, 17 May 2019 15:39:50 input_data.py[line:81] INFO u_list len = 619030
Fri, 17 May 2019 15:40:01 input_data.py[line:103] INFO adj_batch.shape = (10, 619030)
Fri, 17 May 2019 15:40:05 input_data.py[line:106] INFO len(features) = 619030
Fri, 17 May 2019 15:40:05 input_data.py[line:108] INFO ######### Data loaded ###########
Fri, 17 May 2019 15:40:06 train.py[line:51] INFO num_features = 8
Fri, 17 May 2019 15:40:06 train.py[line:62] INFO create model
Fri, 17 May 2019 15:40:07 deprecation.py[line:323] WARNING From /Users/chaoyanghe/Presentation/10.Federated-Learning-with-DPASGD/sourcecode/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Fri, 17 May 2019 15:40:07 deprecation.py[line:506] WARNING From /Users/chaoyanghe/Presentation/12.GraphNeuralNetwork/sourcecode/bipartite-graph-learning/gae/layers.py:82: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Fri, 17 May 2019 15:40:07 model.py[line:116] INFO Finish calculating the latent vector!!!!!!!!!
Fri, 17 May 2019 15:40:07 model.py[line:89] INFO GCNVAE model n_samples = 1
Fri, 17 May 2019 15:40:07 model.py[line:90] INFO GCNVAE model initialized!!!!!!!!
Fri, 17 May 2019 15:40:07 train.py[line:73] INFO pos_weight = 3993.306427330945
Fri, 17 May 2019 15:40:07 train.py[line:76] INFO norm = 0.5001252095247632
Fri, 17 May 2019 15:40:07 train.py[line:81] INFO optimizer
Fri, 17 May 2019 15:40:08 deprecation.py[line:323] WARNING From /Users/chaoyanghe/Presentation/10.Federated-Learning-with-DPASGD/sourcecode/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Fri, 17 May 2019 15:40:08 train.py[line:97] INFO initialize session
Fri, 17 May 2019 15:40:08 train.py[line:127] INFO preprocessing data
Fri, 17 May 2019 15:40:36 train.py[line:131] INFO done preprocessing data.
Fri, 17 May 2019 15:40:37 train.py[line:143] INFO features[2] = (619030, 8)
Fri, 17 May 2019 15:40:37 train.py[line:145] INFO num_features = 8
Fri, 17 May 2019 15:40:37 train.py[line:147] INFO features_nonzero = 4050
Fri, 17 May 2019 15:40:37 train.py[line:153] INFO train model
Fri, 17 May 2019 15:40:37 train.py[line:156] INFO construct dictionary
Fri, 17 May 2019 15:40:37 train.py[line:161] INFO adj_norm_batch = (array([[     0,      0],
       [     0,     16],
       [     0,     51],
       ...,
       [     0, 527691],
       [     0, 603381],
       [     0, 608378]], dtype=int32), array([0.0019685 , 0.0019608 , 0.0018297 , 0.00217531, 0.00224091,
       0.00188842, 0.00196272, 0.00254886, 0.00108897, 0.00107545,
       0.00199419, 0.00153541, 0.00143271, 0.00146756, 0.00182659,
       0.00167695, 0.00167456, 0.00211756, 0.0016332 , 0.00140726,
       0.00163988, 0.00162225, 0.00173891, 0.00285798, 0.00150421,
       0.00183126, 0.00182659, 0.00169893, 0.00151381, 0.00193269,
       0.00158356, 0.00247637, 0.00191283, 0.00244237, 0.00137513,
       0.00175516, 0.00200026, 0.00187154, 0.00150248, 0.00153266,
       0.00170772, 0.00126714, 0.00148554, 0.00154751, 0.00143496,
       0.00296445, 0.0012867 , 0.00140726, 0.00165006, 0.00136857,
       0.00144482, 0.00139538, 0.00150594, 0.00147811, 0.00411945,
       0.00220738, 0.00278938, 0.00142237, 0.00207091, 0.00321878,
       0.00176486, 0.00195129, 0.00179935, 0.00166509, 0.0017497 ,
       0.00175242, 0.00168539, 0.00194753, 0.00140024, 0.00167456,
       0.00190401, 0.00138044, 0.00231598, 0.00280607, 0.0019763 ,
       0.00157159, 0.00141873, 0.01478928, 0.00170018, 0.00150942,
       0.0021056 , 0.00203788, 0.001418  , 0.00312947, 0.00221287,
       0.00136857, 0.00162333, 0.0012249 , 0.00173758, 0.00199419,
       0.00149394, 0.00373644, 0.00147159, 0.00190401, 0.00131406,
       0.00186657, 0.0018189 , 0.00157754, 0.00143646, 0.0021056 ,
       0.00186163, 0.00145645, 0.00089129, 0.00140374, 0.0016277 ,
       0.00441476, 0.00186822, 0.0019685 , 0.00204872, 0.00136987,
       0.00192359, 0.00169274, 0.00172833, 0.00170645, 0.00229114,
       0.00227902, 0.00162225, 0.00145176, 0.00182505, 0.00190576,
       0.00256586, 0.00154003, 0.00226709, 0.00149649, 0.00151205,
       0.00180083, 0.00194753, 0.00125341, 0.00317725, 0.00183282,
       0.00145332, 0.00207544, 0.00179935, 0.00168418, 0.00121294,
       0.00170645, 0.00179788, 0.00156085, 0.00317725, 0.00151469,
       0.00200844, 0.00143346, 0.0017231 , 0.00132812, 0.00201257,
       0.00191283, 0.00169769, 0.00160728, 0.00278938, 0.00126149,
       0.0018098 , 0.00190929, 0.00164778, 0.00178474, 0.00157356,
       0.00318542, 0.00268526, 0.00229114, 0.00189703, 0.00224954,
       0.00183911, 0.00168297, 0.00200638, 0.00170519, 0.00235812,
       0.0013621 , 0.00265623, 0.00088453, 0.00177047, 0.0020892 ,
       0.00281736, 0.00169151, 0.00191819, 0.00233515, 0.00158054,
       0.00165349, 0.00134882, 0.001296  , 0.001792  , 0.00232551,
       0.00245731, 0.00222396, 0.00139676, 0.00182043, 0.00248802,
       0.00220738, 0.00168539, 0.00201257, 0.00149225, 0.00156864,
       0.00165926, 0.00152002, 0.00173095, 0.00154003, 0.00211036,
       0.00142091, 0.00213465, 0.00170268, 0.00212483, 0.00202722,
       0.00142973, 0.00178329, 0.00197044, 0.00174293, 0.0066887 ,
       0.00171793, 0.00218056, 0.00202934, 0.00264676, 0.00321878,
       0.0017231 , 0.00204654, 0.00200844, 0.00163653, 0.00160203,
       0.002592  , 0.0019254 , 0.00184866, 0.00267548, 0.00310637,
       0.00277842, 0.00210088, 0.00170018, 0.00194379, 0.00155701,
       0.0018083 , 0.00205751, 0.00229728, 0.00380451, 0.00251992,
       0.00157654, 0.00150768, 0.00191819, 0.00188842, 0.00171408,
       0.00174698, 0.00710454, 0.00152993, 0.0016581 , 0.00092373,
       0.0017593 , 0.00194193, 0.00193637, 0.00253221, 0.00316108,
       0.00219385, 0.00169521, 0.0019254 , 0.00185836, 0.00142677,
       0.00173095, 0.00149057, 0.00135384, 0.00183911, 0.00174159,
       0.00186822, 0.00233839, 0.00155605, 0.00240618, 0.00206195,
       0.00171793, 0.00174834, 0.00148638, 0.00187321, 0.00161045,
       0.00134386, 0.00175792, 0.0019763 , 0.00221287, 0.00160203,
       0.00219923, 0.00177471, 0.00194007, 0.00231914, 0.00320197,
       0.00332551, 0.00120353, 0.0020209 , 0.00136792, 0.00153175,
       0.00238179, 0.00191461, 0.00168783, 0.00226414, 0.00184386,
       0.00274105, 0.00249589, 0.00171536, 0.0039526 , 0.00194007,
       0.00267548, 0.00123435, 0.00256158, 0.00189185, 0.00197239,
       0.00175379, 0.00235812, 0.00183753, 0.00179493, 0.00492976,
       0.00186822, 0.00193086, 0.00147975, 0.00190051, 0.00162879,
       0.00487   , 0.00167099, 0.00171025, 0.00194941, 0.00177899,
       0.0017231 , 0.00260987, 0.00164438, 0.00220194, 0.00260537,
       0.00208229, 0.00148554, 0.00230036, 0.00239914, 0.00216493,
       0.00164892, 0.00165006, 0.00105999, 0.00157854, 0.00272549,
       0.00153541, 0.00177756, 0.00183753, 0.00169028, 0.00164892,
       0.0028881 , 0.00166981, 0.00215979, 0.00231914, 0.00184706,
       0.00188842, 0.00238523, 0.00171153, 0.00208458, 0.00190051,
       0.00318542, 0.00228506, 0.00124499, 0.00405021, 0.00164664,
       0.00246869, 0.00168055, 0.00296445, 0.0020509 , 0.00739464,
       0.00188331, 0.00236481, 0.00187656, 0.00293191, 0.00179346,
       0.00131176, 0.00181131, 0.00173227, 0.00196272, 0.00273065,
       0.00289424, 0.00123292, 0.00215469, 0.00151558, 0.00199419,
       0.00266101, 0.00221562, 0.00374977, 0.00160203, 0.00316913,
       0.00176346, 0.0020531 , 0.00222957, 0.00306167, 0.00175379,
       0.00248412, 0.0029845 , 0.00235149, 0.00248024, 0.00084776,
       0.00191283, 0.00246488, 0.00169274, 0.00198023, 0.0022881 ,
       0.00270014, 0.00190576, 0.00154563, 0.00519286, 0.00119608,
       0.00089857, 0.00240973, 0.00149991, 0.00186657, 0.0012263 ,
       0.00222676, 0.00289424, 0.00176766, 0.00195508, 0.00285207,
       0.00242046, 0.00219653, 0.0013305 , 0.001641  , 0.00185027,
       0.00181434, 0.00280607, 0.00280047, 0.0018068 , 0.00297109,
       0.00275158, 0.00273065, 0.00255308, 0.00194007, 0.00170898,
       0.00287594, 0.00199017, 0.00270515, 0.00181434, 0.00301188,
       0.00182197, 0.00194753, 0.00475673, 0.00424967, 0.00299128,
       0.00239564, 0.00174025, 0.00222957, 0.00249195, 0.00558982,
       0.00257883, 0.0028881 , 0.00200844, 0.00227602, 0.00275158,
       0.00222957, 0.00226414, 0.00237837, 0.00178042, 0.00167575,
       0.00227006, 0.00348587, 0.00223522, 0.00246488, 0.00502367,
       0.00238179, 0.00088208, 0.00225534, 0.00291919, 0.00526549,
       0.002592  , 0.00558982, 0.00226414, 0.0035186 , 0.00134696,
       0.00445913, 0.00338301, 0.00465101, 0.00419237, 0.00363475,
       0.00598256, 0.00558982, 0.0028117 , 0.00284036, 0.00297777,
       0.00208689, 0.00558982, 0.00304719, 0.00499177, 0.0029448 ,
       0.00393701, 0.00368455, 0.00376323, 0.00241329, 0.00343329,
       0.00280607, 0.0029981 , 0.00640394, 0.00215469, 0.0118578 ,
       0.00424967, 0.00289424, 0.00362262, 0.00445913, 0.00423031,
       0.00302586, 0.00156571, 0.00432986, 0.00207317, 0.00236818,
       0.00311401, 0.002524  , 0.00423031, 0.00258319, 0.0068461 ,
       0.00550315, 0.00489961, 0.0036594 , 0.00285207, 0.0035186 ,
       0.01676946, 0.00499177, 0.00445913, 0.00499177, 0.00415543,
       0.00499177, 0.00719741, 0.00502367]), (1, 619030))
Fri, 17 May 2019 15:40:37 train.py[line:165] INFO adj_label_batch = (array([[     0,      0],
       [     0,     16],
       [     0,     51],
       ...,
       [     0, 527691],
       [     0, 603381],
       [     0, 608378]], dtype=int32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), (1, 619030))
Fri, 17 May 2019 15:40:37 train.py[line:169] INFO update
Fri, 17 May 2019 15:40:37 train.py[line:172] INFO run the model
Fri, 17 May 2019 15:40:37 train.py[line:174] INFO outs = [array([[0.        , 0.6961275 , 0.        , 0.        , 0.28665972,
        0.0712156 , 0.4873386 , 0.41901386, 0.        , 0.5266902 ,
        0.        , 0.        , 0.        , 0.        , 1.429125  ,
        0.        , 0.        , 0.        , 0.5444163 , 0.        ,
        0.64303195, 0.49334753, 0.4590683 , 0.        , 0.27708203,
        0.72327965, 0.14087334, 0.37833315, 0.        , 1.3991194 ,
        0.        , 0.        ]], dtype=float32)]
Fri, 17 May 2019 15:40:37 train.py[line:178] INFO average loss
